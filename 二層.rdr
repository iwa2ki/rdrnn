「ニューラルネットワーク.rdr」を参照する。
二層とは
	＋パラメータ：配列
	【層】は，｛｝。
	はじめ（入力の大きさ，隠れ層の大きさ，出力の大きさ）の手順
		パラメータ（1）は，行列（入力の大きさ，隠れ層の大きさ）を作ったもの。
		パラメータ（1）をランダム化する。
		パラメータ（2）は，行列（1，隠れ層の大きさ）を作ったもの。
		パラメータ（3）は，行列（隠れ層の大きさ，出力の大きさ）を作ったもの。
		パラメータ（3）をランダム化する。
		パラメータ（4）は，行列（1，出力の大きさ）を作ったもの。
		層（1）は，全結合層（パラメータ（1），パラメータ（2））を作ったもの。
		層（2）は，ランプ関数を作ったもの。
		層（3）は，全結合層（パラメータ（3），パラメータ（4））を作ったもの。
		層（4）は，ソフトマックスに損失関数もくっつけたやつを作ったもの。
	終わり
	自分で【入力：行列】から推定する手順
		（層の個数-1）回，【カウンタ】にカウントして繰り返す。
			入力は，層（カウンタ）が入力を順伝播させたもの。
		繰り返し終わり。
		入力を返す。
	終わり
	自分で【正解：行列】が正解で【入力：行列】における損失を求める手順
		【推定結果】は，自分で入力から推定したもの
		層（層の個数）が推定結果を正解で順伝播させたものを返す。
	終わり
	自分で【正解：行列】が正解で【入力：行列】における勾配を計算する手順
		自分で正解が正解で入力における損失。
		【出力】は，層（層の個数）が逆伝播させたもの。
		（層の個数-1）回，【カウンタ】にカウントして繰り返す
			出力は，層（層の個数-カウンタ）が出力を逆伝播させたもの。
		繰り返し終わり
		【勾配：配列】。
		勾配は，｛｝。
		勾配（1）は，層（1）の行列dW。
		勾配（2）は，層（1）の行列dB。
		勾配（3）は，層（3）の行列dW。
		勾配（4）は，層（3）の行列dB。
		勾配を返す。
	終わり
終わり
